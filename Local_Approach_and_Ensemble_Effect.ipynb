{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b0c0e23-abbb-4131-82e8-ab9e2c732a46",
     "showTitle": false,
     "title": ""
    },
    "id": "ZKj_De-_bDAj"
   },
   "outputs": [],
   "source": [
    "# Intializing Spark Session\n",
    "! pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"MLlib lab\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6310213-1d20-4d8f-8d8f-6afc65232127",
     "showTitle": false,
     "title": ""
    },
    "id": "dPBLFGLZbFgw"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "012fb92a-6026-4340-aa04-0b295b2c378f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vxX4SqobJSs",
    "outputId": "29e1ee9d-6643-41c0-bbc9-8778711ff9f8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:1026034844\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampNTZType, LongType, DoubleType,IntegerType\n",
    "\n",
    "parquet_files = '/content/drive/MyDrive/fhvhv_tripdata_2024-01.parquet'\n",
    "schema = StructType([\n",
    "  StructField('hvfhs_license_num', StringType(), nullable=True),\n",
    "  StructField('dispatching_base_num', StringType(), nullable=True),\n",
    "  StructField('originating_base_num', StringType(), nullable=True),\n",
    "  StructField('request_datetime', TimestampNTZType(), nullable=True),\n",
    "  StructField('on_scene_datetime', TimestampNTZType(), nullable=True),\n",
    "  StructField('pickup_datetime', TimestampNTZType(), nullable=True),\n",
    "  StructField('dropoff_datetime', TimestampNTZType(), nullable=True),\n",
    "  StructField('PULocationID', LongType(), nullable=True),\n",
    "  StructField('DOLocationID', LongType(), nullable=True),\n",
    "  StructField('trip_miles', DoubleType(), nullable=True),\n",
    "  StructField('trip_time', LongType(), nullable=True),\n",
    "  StructField('base_passenger_fare', DoubleType(), nullable=True),\n",
    "  StructField('tolls', DoubleType(), nullable=True),\n",
    "  StructField('bcf', DoubleType(), nullable=True),\n",
    "  StructField('sales_tax', DoubleType(), nullable=True),\n",
    "  StructField('congestion_surcharge', DoubleType(), nullable=True),\n",
    "  StructField('airport_fee', IntegerType(), nullable=True),\n",
    "  StructField('tips', DoubleType(), nullable=True),\n",
    "  StructField('driver_pay', DoubleType(), nullable=True),\n",
    "  StructField('shared_request_flag', StringType(), nullable=True),\n",
    "  StructField('shared_match_flag', StringType(), nullable=True),\n",
    "  StructField('access_a_ride_flag', StringType(), nullable=True),\n",
    "  StructField('wav_request_flag', StringType(), nullable=True),\n",
    "  StructField('wav_match_flag', IntegerType(), nullable=True)\n",
    "])\n",
    "# Create empty dataframe\n",
    "union_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "# Process each Parquet file\n",
    "for file in parquet_files:\n",
    "  if 'parquet' in parquet_files:\n",
    "    df = spark.read.parquet(parquet_files)\n",
    "    # Read the Parquet file with schema inference\n",
    "  #df = spark.read.parquet(file.path)\n",
    "\n",
    "    # (column with mismatch, desirable type)\n",
    "  mismatch_col = [\n",
    "      (\"wav_match_flag\", \"string\"),\n",
    "      (\"airport_fee\", \"double\"),\n",
    "      (\"PULocationID\", \"bigint\"),\n",
    "      (\"DOLocationID\", \"bigint\")\n",
    "    ]\n",
    "\n",
    "  df = df.withColumns({c: F.col(c).cast(t) for c, t in mismatch_col})\n",
    "\n",
    "    # Union the casted DataFrame with the union_df\n",
    "  union_df = union_df.union(df)\n",
    "\n",
    "# Revert the variable name after unioning the whole data\n",
    "df = union_df\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows:{total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb346a6f-7159-450e-8df3-ca072d327c5d",
     "showTitle": false,
     "title": ""
    },
    "id": "I2ANlAmrbdIw"
   },
   "outputs": [],
   "source": [
    "#df=df.withColumn(\"day\",F.dayofyear('pickup_datetime'))\n",
    "df=df.withColumn(\"trip_meters\",F.col('trip_miles')*1609.35)\n",
    "df=df.withColumn(\"hour\",F.hour('pickup_datetime'))\n",
    "#df=df.withColumn(\"month\",F.month('pickup_datetime'))\n",
    "df=df.withColumn(\"Date\",F.to_date('pickup_datetime'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c8f6e12-8f6b-4879-be07-e19259e37d30",
     "showTitle": false,
     "title": ""
    },
    "id": "uRqJ3fQ2bv55"
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "year_range = (2019, 2025)\n",
    "hds = []\n",
    "for y in range(year_range[0], year_range[1]):\n",
    "  hds += holidays.US(state=\"NY\", years=y).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c2cce8c-18cb-4dcc-8f64-1e1815ff4120",
     "showTitle": false,
     "title": ""
    },
    "id": "bpGFy-ncbyTH"
   },
   "outputs": [],
   "source": [
    "df = df.withColumns({\n",
    "  \"isWeekend\": F.when(F.col(\"Date\").isin(hds) | F.dayofweek(F.col(\"Date\")).isin([1, 7]), 1).otherwise(0),\n",
    "  \"isOvernight\": F.when(F.col(\"hour\").isin(list(range(20, 24))+list(range(0, 6))), 1).otherwise(0)}) \\\n",
    "  .withColumn(\"isRushhour\",\n",
    "              F.when(F.col(\"hour\").isin(list(range(16, 20))) | (F.col(\"isWeekend\") == 0), 1).otherwise(0)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa11f17e-e729-45c2-bb49-18cd33639975",
     "showTitle": false,
     "title": ""
    },
    "id": "wyWIHVOWbll3"
   },
   "outputs": [],
   "source": [
    "drop = ['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge',\n",
    " 'airport_fee', 'tips', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag','trip_miles','Date']\n",
    "df = df.drop(*drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd14e30-7090-4365-8622-7b74bc09407b",
     "showTitle": false,
     "title": ""
    },
    "id": "iMNAp6R7b-nH"
   },
   "outputs": [],
   "source": [
    "df = df.select('trip_time', 'trip_meters', 'hour', 'isWeekend', 'isOvernight', 'isRushhour', 'base_passenger_fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4709392-0a28-4ef9-8b4d-1c24e347ee33",
     "showTitle": false,
     "title": ""
    },
    "id": "F_dY56OWb9md"
   },
   "outputs": [],
   "source": [
    "df1 =df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c40753e1-c87f-40b4-9e20-df37d6858fcd",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yWvNI7FNcAY0",
    "outputId": "c9357fd9-fbfc-40c1-8f40-09db4ea350a4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size: 718214235\nTest Data Size: 307820609\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "# Set the random seed (for reproducibility)\n",
    "seed = 42\n",
    "# Split the DataFrame into training and test sets using a fixed random split\n",
    "train_df, test_df = df1.randomSplit([0.7, 0.3], seed=seed)\n",
    "# Print the sizes of the training and test sets\n",
    "print(f\"Training Data Size: {train_df.count()}\")\n",
    "print(f\"Test Data Size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "009055be-5774-44b7-8e4a-f5d93ed4a83f",
     "showTitle": false,
     "title": ""
    },
    "id": "c-M-SdLucIrZ"
   },
   "outputs": [],
   "source": [
    "train_rdd = train_df.rdd.repartition(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8890eba-d70f-46b2-961e-16ba772ba13e",
     "showTitle": false,
     "title": ""
    },
    "id": "5CHaQvlgcLS7"
   },
   "outputs": [],
   "source": [
    "test_rdd = test_df.rdd.repartition(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fcb1cf0-03e3-4126-8b05-a322800d5a7f",
     "showTitle": false,
     "title": ""
    },
    "id": "ntMGoz6QcOHE"
   },
   "outputs": [],
   "source": [
    "column_names = df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b27be848-ef5d-4e9a-a7c6-898e4ad30f1d",
     "showTitle": false,
     "title": ""
    },
    "id": "1yv1Nj-lcVJl"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "def build_model(partition_data_it):\n",
    "  partition_data_df = pd.DataFrame(partition_data_it,columns=column_names)\n",
    "  reg = DecisionTreeRegressor(max_depth=5)\n",
    "  X_train = partition_data_df[['trip_time', 'trip_meters', 'hour', 'isWeekend', 'isOvernight', 'isRushhour']]\n",
    "  y_train = partition_data_df[\"base_passenger_fare\"]\n",
    "  model = reg.fit(X_train.values,y_train.values)\n",
    "  return [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "404e17c2-5c9d-418e-a201-4ddbd860c1a0",
     "showTitle": false,
     "title": ""
    },
    "id": "OCU-5zdKcX0H"
   },
   "outputs": [],
   "source": [
    "models = train_rdd.mapPartitions(build_model).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3face8b6-245e-4bc8-be2b-22467c087171",
     "showTitle": false,
     "title": ""
    },
    "id": "KtV3tIVFetOJ"
   },
   "outputs": [],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22dd278c-fce8-4bf7-b874-9ff41834cd50",
     "showTitle": false,
     "title": ""
    },
    "id": "MDorFV6Mczf2"
   },
   "outputs": [],
   "source": [
    "def predict(instance):\n",
    "  return[m.predict([instance[:-1]])[0] for m in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ae2901-9356-4f2f-bffb-d92a917645ec",
     "showTitle": false,
     "title": ""
    },
    "id": "o2htKV4HdFcV"
   },
   "outputs": [],
   "source": [
    "test_rdd.map(predict).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef53d742-794e-4a74-99fb-5f5c7d132cd6",
     "showTitle": false,
     "title": ""
    },
    "id": "A0B2MFPKjeir"
   },
   "outputs": [],
   "source": [
    "def agg_predictions(preds):\n",
    "  mean = sum(preds)/ len(preds)\n",
    "  return float(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0aa23ad5-3a14-475d-85bf-4fd5283bc2bc",
     "showTitle": false,
     "title": ""
    },
    "id": "Hyun-wS1jiFh"
   },
   "outputs": [],
   "source": [
    "test_rdd.map(predict).map(agg_predictions).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ceb38f9f-5404-45df-a51f-6d41e0489b3c",
     "showTitle": false,
     "title": ""
    },
    "id": "w6-0AyyjjnVG"
   },
   "outputs": [],
   "source": [
    "def transform(instance):\n",
    "  return Row(**instance.asDict(), \\\n",
    "             raw_prediction = agg_predictions(predict(instance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f2c7647-a647-4b1e-b982-b36411357839",
     "showTitle": false,
     "title": ""
    },
    "id": "yoIJBdf_jr83"
   },
   "outputs": [],
   "source": [
    "predictions = test_rdd.map(transform).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6afb1ee0-74b9-4dbd-9c5b-06c7594ed429",
     "showTitle": false,
     "title": ""
    },
    "id": "yS4Ovmuo0KE-"
   },
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de4e4922-57b6-481b-9ef6-914669416e61",
     "showTitle": false,
     "title": ""
    },
    "id": "r6LdsnBA1vrd"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\",labelCol='base_passenger_fare',predictionCol=\"raw_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf381345-7895-4d89-8200-86e9c21cd9c7",
     "showTitle": false,
     "title": ""
    },
    "id": "rYZdaRgO2OrQ"
   },
   "outputs": [],
   "source": [
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fc997cf-7650-43d9-a9e5-2b86e68703c6",
     "showTitle": false,
     "title": ""
    },
    "id": "-CpCeTGF2ss3"
   },
   "outputs": [],
   "source": [
    "partitions = [1,2,4,8,16,32,64,128,256,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38c812a9-55c3-41d4-9091-548286841033",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OEHLR1na2qYW",
    "outputId": "054dca6f-b5e3-45b8-ea88-246bbf6af1f1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error of the model with 1 partitions: 9.658999559247526\nTime Required to run in seconds: 27.397446393966675\nRoot Mean Square Error of the model with 2 partitions: 9.51505236024468\nTime Required to run in seconds: 27.85700821876526\nRoot Mean Square Error of the model with 4 partitions: 9.475726336789966\nTime Required to run in seconds: 33.78821897506714\nRoot Mean Square Error of the model with 8 partitions: 9.365018043951174\nTime Required to run in seconds: 37.46826457977295\nRoot Mean Square Error of the model with 16 partitions: 9.345751586720935\nTime Required to run in seconds: 57.52696967124939\nRoot Mean Square Error of the model with 32 partitions: 9.343354247297578\nTime Required to run in seconds: 84.19667625427246\nRoot Mean Square Error of the model with 64 partitions: 9.381641885933675\nTime Required to run in seconds: 143.88914561271667\nRoot Mean Square Error of the model with 128 partitions: 9.566239350984334\nTime Required to run in seconds: 262.69355821609497\nRoot Mean Square Error of the model with 256 partitions: 9.903724030871782\nTime Required to run in seconds: 494.5993220806122\nRoot Mean Square Error of the model with 512 partitions: 10.651567704755154\nTime Required to run in seconds: 963.8303971290588\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in partitions:\n",
    "  start = time.time()\n",
    "  train_rdd = train_df.rdd.repartition(i)\n",
    "  models = train_rdd.mapPartitions(build_model).collect()\n",
    "  predictions = test_rdd.map(transform).toDF()\n",
    "  end = time.time()\n",
    "  with open(\"Local_Ensemble_dt.csv\", \"a\") as f:\n",
    "        print(\n",
    "              f\"{i},{evaluator.evaluate(predictions)},{end - start}\",\n",
    "              file=f,\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4952b50f-2063-4a28-8b8b-4a36812b9514",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZ8YaUVMHgyz",
    "outputId": "50099438-ce7a-444d-9a21-f438e1bd41dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error of the model with 1 partitions: 15.524996797170678\nTime Required to run in seconds: 44.71863126754761\nRoot Mean Square Error of the model with 2 partitions: 12.128528200779945\nTime Required to run in seconds: 52.64550304412842\nRoot Mean Square Error of the model with 3 partitions: 10.025461163313398\nTime Required to run in seconds: 64.19073796272278\nRoot Mean Square Error of the model with 4 partitions: 9.570910653161535\nTime Required to run in seconds: 67.51734042167664\nRoot Mean Square Error of the model with 5 partitions: 9.320390831927257\nTime Required to run in seconds: 70.78808665275574\nRoot Mean Square Error of the model with 6 partitions: 9.397083225926577\nTime Required to run in seconds: 71.75579500198364\nRoot Mean Square Error of the model with 7 partitions: 9.375406942406496\nTime Required to run in seconds: 77.8984055519104\nRoot Mean Square Error of the model with 8 partitions: 9.33079364868918\nTime Required to run in seconds: 73.29802751541138\nRoot Mean Square Error of the model with 9 partitions: 9.357210937820822\nTime Required to run in seconds: 79.97506141662598\nRoot Mean Square Error of the model with 10 partitions: 9.349671333417808\nTime Required to run in seconds: 83.89661598205566\n"
     ]
    }
   ],
   "source": [
    "for j in range(1,11):\n",
    "  def build_model(partition_data_it):\n",
    "    partition_data_df = pd.DataFrame(partition_data_it,columns=column_names)\n",
    "    reg = DecisionTreeRegressor(max_depth=j)\n",
    "    X_train = partition_data_df[['trip_time', 'trip_meters', 'hour', 'isWeekend', 'isOvernight', 'isRushhour']]\n",
    "    y_train = partition_data_df[\"base_passenger_fare\"]\n",
    "    model = reg.fit(X_train.values,y_train.values)\n",
    "    return [model]\n",
    "  start_time = time.time()\n",
    "  train_rdd = train_df.rdd.repartition(j)\n",
    "  models = train_rdd.mapPartitions(build_model).collect()\n",
    "  predictions = test_rdd.map(transform).toDF()\n",
    "  print(f\"Root Mean Square Error of the model with \" f\"{j} partitions: {evaluator.evaluate(predictions)}\")\n",
    "  end_time= time.time()\n",
    "  print(f\"Time Required to run in seconds: {end_time-start_time}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "Local_Approach_and_Ensemble_Effect",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
